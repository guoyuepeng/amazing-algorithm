### 并行训练平台
- MPI
- XGBOOST
- Parameter Server
- MxNet
- TenserFlow	

### 特征工程
## 特征没做好，参数调到老
- 缺失值处理
	- “是否缺失”作为一个变量
	- 根据业务经验填充，$重要变量尽量填充，避免做决策树模型时因为该值null，分到不合理的节点$
	- 模型预测缺失值（sknn、svd、Bayes+EM）
		- sknn
		最近k个邻（3,5）
		距离平方的倒数为权
		从缺失最少的开始
		填充数据可以用于后续计算
		- svd
		寻找与数据X同大小填补矩阵Y
		最小化Φ=‖X-eTμ-Y‖
		SVD迭代求解
		- EM
		选取初始簇中心
		E步：贝叶斯最大后验簇分配
		M步：重新分配簇
- 无效特征丢弃
	- 缺失值过多
	- 方差过小：基本不变
- 异常值处理
	- 基于统计的异常点检测算法
		- 3sigma：与平均值超过3倍标准差，数据需满足正态分布
 			箱线图：大于QU+1.5IQR,小于QL−1.5IQR.QU:上四分位数,QL:下四分位数,IQR:QU-QL.
	- 基于距离的异常点检测算法
		主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离 ( 曼哈顿距离 ) 、欧氏距离和马氏距离等方法。
	- 基于密度的异常点检测算法
	考察当前点周围密度，可以发现局部异常点，例如LOF算法
	- 异常检测算法（孤立森林，多元高斯分布）
		- 孤立森林（Isolation Forest）(skelarn)
		样本抽样（256）
		分类树 -> 森林
		计算样本所处节点深度
		平均深度越小越异常
		- 多元高斯分布
		选择特征去均值化
		学习高斯混合分布协方差矩阵
		计算样本出的高斯分布概率值

- 特征变换
	- x1/x2,x1*x2,x1^n
	- log变换(也可用在y变量上)：压缩了数据的尺度，使得数据更加平稳，削弱了模型的共线性、异方差性
	- box-cox变换
	- 连续值离散化：df.qcut,或者按照业务经验分段
	- 连续值排序：df.rank
	- 按时间偏移：对于部分时间序列变量存在“时滞”效应
	- XGBOOST叶节点
- 特征选择
	- RF/xgboost等树模型
	- L1正则
	- 求自变量和因变量的相关性
	- sklearn.RFECV

- 数据非平衡
想办法弄到更多的数据
想办法把数据弄平衡 : 重采样（SMOTE）、欠采样，正负样本采样比例=1：4（依不同数据集而定）
想办法调参：min_child_weight 、scale_pos_weight…
AUC很好，线上效果却不怎么好？
-  	拿预测结果与真实值再做一次拟合
-  	换指标—容易被正负样本比例影响的指标：PR曲线的F1 score
原因：在样本比例变得不平衡之后（负样本远多于正样本），ROC曲线变化不大，而PR曲线会有较大的变化，更能正确反映出分类器的性能。
ROC：纵轴—召回率，横轴—假正例率
5. 决策树表现优于神经网络
 	修改代价函数：提高少类样本判错的代价，xgboost—weight（初始化Dmatrix时）

### 模型
- LR
	- 算法简单，容易并行和工程化，FTRL是其实时化版本。能够处理超高维稀疏的样本，
	缺点是基本要靠人工堆特征。线性模型的弊端
	-  	不能很好利用高维度的复杂特征，需要手动添加交叉特征
	-  	用户方面的特征对同一个用户而言毫无意义

- SVM
- knn
- EM
- Naive Bayes
- Tree
	- RF
	- gbdt
	- XGBOOST、LightGBM
	 优点：
		-  	可以有效地探索和转化特征向量空间
		-  	对于单个特征值误差包容度较高
		-  	特征重要性一目了然
		
		缺点：
		-  	模型参数数量按决策树深度几何级数增长
		-  	无法有效利用高维度的稀疏特征（如ID特征，交叉特征等）
			策略：
			1.     将稀疏和非稀疏特征分开
			2.     用非稀疏特征产生迭代决策树模型
			3.     用决策树的叶子接点产生新的特征向量，然后和稀疏特征连接在一起，作为新的特征向量
			4.     用逻辑回归来建立最终的预测模型
		-  	对离散特征的概括性的总结利用不够充分
			1.      只是单纯记忆而非概括性的总结
			2.      神经网络产生离散特征的embedding，再作为决策树的原始特征
		XGBOOST多目标训练，如前100棵树一个目标函数，后面100棵树另一个目标函数
	- ExtraTreesRegressor
- Deep Learning	
	深度学习与xgboost比较
	优势
	n  更大的模型容量，参数更多
	n  更复杂的特征交互
	n  更多的非线性关系
	n  只需要原始特征，不需要手动添加非线性特征
	劣势
	n  对于原始特征质量要求更高
	n  特征的重要性顺序不清晰，更加黑盒化
	n  训练过程更复杂，耗时较长
- 融合
	- bagging：加权平均
	- 级联：模型的中间结果输出到另一个模型
	- Wide & Deep	

### 排序
- learning to rank

### 调参
- bias & variance tradeoff ： 同时观察在训练集和验证集的表现

### 评价指标
- ROC
- ks检验
- MAE、RMSE、MAPE