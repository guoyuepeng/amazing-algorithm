-  https://mooc.study.163.com/learn/2001281002?tid=2001392029#/learn/content?type=detail&id=2001702003&cid=2001699108


## 1. Neural Networks and Deep Learning

### week1. 深度学习概论
- 驱动深度学习发展的因素
    - data
    - computation
    - algorithms

### week2. 神经网络基础
2.1 符号说明
- 输入样本矩阵：(nx,m), 一列是一个样板-- nx个维度/特征，m个样本
  Y矩阵:1*m

2.2 逻辑回归
- 将值转换到(0,1)区间
- 损失函数(loss function)
    - 衡量在单个样本上的表现
    - 凸函数，不然会得到很多个局部最优解(所以不能用平方误差：$\frac{1}{2}(y^--y)^2$)
    $$ L = - (y*log(y^-)+(1-y)log(1-y^-)) $$
    - $y^-$是预测值，y是真实值
    - 损失函数越小越好
- 成本函数（cost function）
    - 衡量在全体训练样本上的表现
    $$J(w,b) = \frac{1}{m}\sum_i^m(L(y-^i,y^i))) $$ 
    - 推导：最大似然估计（给定样本的观测值概率最大）
        - 假设训练集样本都是独立同分布
- 梯度下降法(Gradient Descent)
    - 参数初始化
    - 参数朝梯度下降的方向迭代，使得cost function朝全局最优解收敛

 2.3 导数   
 - 即斜率
 - 计算图(Computation Graph)
 - 反向传播：链式法则（chain rule）

2.4 向量化(vectorization)
- for循环很低效，效率差百倍
- ```np.dot(w,x)```
- GPU，CPU 都有并行化指令(SIMD),numpy可以利用并行化，尽量使用numpy的内置函数计算

2.5 python广播(Broadcasting)
- 计算时将维度较少的变量自动展开

2.6 python/numpy,debug技巧
- 定义向量时
don't use: ```np.random.randn(5)```
use:```np.random.randn(5,1) or np.random.randn(1,5)``` 
- 通过assert 确定使用的是n×1、1×n的矩阵
- np.sum(...,keepdims=Ture):防止输出秩为1的数组

### week3. 浅层神经网络
3.1 概念
- input layer （第0层）
- hidden layer
- output layer

3.2 激活函数
- 为什么需要激活函数
    - 如果没有非线性激活函数，输出就是输入的线性函数
    - 输出层是唯一可以用线性激活函数的地方
- sigmod
    - 适合做输出层的激活函数（0/1分类）
- tanh
    - 优于sigmod
- ReLU
    - 主流
    - 会使得梯度下降速度加快
    - 改进版：leaking ReLU，取负数时斜率不为0

3.3 反向传播算法(BP)
- 前向传递输入信号直至输出产生误差，反向传播误差信息更新权重矩阵
- 梯度下降在链式法则中的应用

3.4 随机初始化
- 不要全零


### week4. 深层神经网络



