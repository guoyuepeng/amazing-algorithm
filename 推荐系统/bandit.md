1. 理论简介
多臂老虎机（Multi-armed Bandit）问题的提出和研究最早可以追述到上世纪三十年代，该问题可以使用一种较为形象的场景进行解释：
假设你进入了一家赌场，这家赌场的大厅里有n个老虎机，当你往老虎机中投入一枚硬币，老虎机有一定概率掉落奖励，每个老虎机掉落奖励的概率不同且未知，那么在现有硬币有限的情况下，使用什么策略才能使你最后所得的总体奖励最大？
在尝试了几次后，你对每个赌博机的掉落概率有了一个初步的估计，但是受尝试次数的限制，这个估计可能是不准确的，且未来该概率还有可能改变。$$接下来，是选择一直坚持目前已知的最好选择（exploitation，守成），还是继续尝试，以达到更准确的估计，找到奖励概率更高的老虎机（exploration，探索），这便是多臂老虎机理论解决的问题。$$通过一系列在线学习反馈算法，平衡守成与探索的比例，以达到最后总体收益的最大化。

这个多臂问题，它是一个可以装下很多问题的万能框：

- 假设一个用户对不同类别的内容感兴趣程度不同，那么我们的推荐系统初次见到这个用户时，怎么快速地知道他对每类内容的感兴趣程度？这就是推荐系统的冷启动。

- 假设我们有若干广告库存，怎么知道该给每个用户展示哪个广告，从而获得最大的点击收益？是每次都挑效果最好那个么？那么新广告如何才有出头之日？

- 我们的算法工程师又想出了新的模型，有没有比A/B test更快的方法知道它和旧模型相比谁更靠谱？

特别提出，在计算广告和推荐系统领域，针对这个问题，还有个说法叫做EE问题：exploit－explore问题。

>exploit意思就是：比较确定的兴趣，当然要用啊。好比说我们已经挣到的钱，当然要花啊；
explore意思就是：不断探索用户新的兴趣才行，不然很快就会出现一模一样的反复推荐。就好比我们虽然有一点钱可以花了，但是还得继续搬砖挣钱啊，不然花完了喝西北风啊。

一切通过数据收集而得到的概率预估任务，都能通过Bandit系列算法来进行在线优化。这里的“在线”，指的不是互联网意义上的线上，而是只算法模型参数根据观察数据不断演变。



2. 策略
- Random
每次随机选择一枚硬币进行投掷。如果不能胜过这个策略，就不必玩了。

- Naive
先给每个硬币一定次数的尝试，比如每个硬币掷10次，根据每个硬币正面朝上的次数，选择正面频率最高的那个硬币，作为最佳策略。这也是大多人能想到的方法。

但是这个策略有几个明显问题：

10次尝试真的靠谱吗？最差的硬币也有可能在这10次内有高于最好硬币的正面次数。
假设你选到的这个硬币在投掷次数多了后发生了问题（比如掉屑），改变了其属性，导致其正面的概率大大降低，如果你还死守着它，那不是吃大亏了？（这是对变量的考虑）
就算你给一个硬币10次机会，如果硬币真的很多，比如K>100，给每个硬币10次机会是不是也太浪费了呢？等所有硬币都尝试过，再回来“赚钱”，花儿都谢了！

- $\epsilon$-Greedy
有了前两个垫背，可以开始让Bandit登场了。ε-Greedy就是一种很机智的Bandit算法：它让每次机会以ε的概率去“探索”，1-ε的概率来“开发”。也即，如果一次机会落入ε中，则随机选择一个硬币来投掷，否则就选择先前探索到正面概率最大的硬币。这个策略有两个好处：

它能够应对变化，如果硬币“变质”了，它也能及时改变策略。
ε-Greedy机制让玩的过程更有趣，有时“探索”，有时“赚钱”。
在此基础上，又能引申出很多值得研究的问题，比如ε应该如何设定呢？它应不应该随着时间而变？因为随着探索次数的增多，好的选择自然浮现得比较明显了。ε大则使得模型有更大的灵活性（能更快的探索到未知，适应变化），ε小则会有更好的稳定性（有更多机会去“开发”）。

- UCB:置信上限法
在统计学中，对于一个未知量的估计，总能找到一种量化其置信度的方法。最普遍的分布正态分布（或曰高斯分布）N(μ,δ)，其中的E就是估计量的期望，而δ则表示其不确定性（δ越大则表示越不可信）。比如你掷一个标准的6面色子，它的平均值是3.5，而如果你只掷一次，比如说到2，那你对平均值的估计只能是2，但是这个置信度应该很低，我们可以知道，这个色子的预估平均值是2，而以95%的置信区间在[1.4,5.2]。

UCB（Upper Confidence Bound - 置信上限）就是以均值的置信上限为来代表它的预估值：

$$ u_i = u_i + 2*\sqrt(\frac{1}{n_i})$$
上面是一个例子，其中μi是对期望的预估，ni是尝试次数，可以看到对i的尝试越多，其预估值与置信上限的差值就越小。也就是越有置信度。

这个策略的好处是，能让没有机会尝试的硬币得到更多尝试的机会，是骡子是马拉出来溜溜！将整个探索+开发的过程融合到一个公式里面，很完美！

- Thompson Sampling


3. 参考资料
- http://blog.findshine.com/2015/01/03/bandit.html
- https://mp.weixin.qq.com/s?__biz=MzA4OTk5OTQzMg==&mid=2449231162&idx=1&sn=701246413bc0448b815803c0dc120f6d&scene=1&srcid=0623bwRRzpaEOOYGiYqYq3bK#rd