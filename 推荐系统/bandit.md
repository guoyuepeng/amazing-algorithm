一. 理论简介
多臂老虎机（Multi-armed Bandit）问题的提出和研究最早可以追述到上世纪三十年代，该问题可以使用一种较为形象的场景进行解释：
假设你进入了一家赌场，这家赌场的大厅里有n个老虎机，当你往老虎机中投入一枚硬币，老虎机有一定概率掉落奖励，每个老虎机掉落奖励的概率不同且未知，那么在现有硬币有限的情况下，使用什么策略才能使你最后所得的总体奖励最大？
在尝试了几次后，你对每个赌博机的掉落概率有了一个初步的估计，但是受尝试次数的限制，这个估计可能是不准确的，且未来该概率还有可能改变。$$接下来，是选择一直坚持目前已知的最好选择（exploitation，守成），还是继续尝试，以达到更准确的估计，找到奖励概率更高的老虎机（exploration，探索），这便是多臂老虎机理论解决的问题。$$通过一系列在线学习反馈算法，平衡守成与探索的比例，以达到最后总体收益的最大化。

这个多臂问题，它是一个可以装下很多问题的万能框：

- 假设一个用户对不同类别的内容感兴趣程度不同，那么我们的推荐系统初次见到这个用户时，怎么快速地知道他对每类内容的感兴趣程度？这就是推荐系统的冷启动。

- 假设我们有若干广告库存，怎么知道该给每个用户展示哪个广告，从而获得最大的点击收益？是每次都挑效果最好那个么？那么新广告如何才有出头之日？

- 我们的算法工程师又想出了新的模型，有没有比A/B test更快的方法知道它和旧模型相比谁更靠谱？

特别提出，在计算广告和推荐系统领域，针对这个问题，还有个说法叫做EE问题：exploit－explore问题。

>exploit意思就是：比较确定的兴趣，当然要用啊。好比说我们已经挣到的钱，当然要花啊；
explore意思就是：不断探索用户新的兴趣才行，不然很快就会出现一模一样的反复推荐。就好比我们虽然有一点钱可以花了，但是还得继续搬砖挣钱啊，不然花完了喝西北风啊。

一切通过数据收集而得到的概率预估任务，都能通过Bandit系列算法来进行在线优化。这里的“在线”，指的不是互联网意义上的线上，而是只算法模型参数根据观察数据不断演变。


二. 策略

1. Random
每次随机选择一枚硬币进行投掷。如果不能胜过这个策略，就不必玩了。

2. Naive
先给每个硬币一定次数的尝试，比如每个硬币掷10次，根据每个硬币正面朝上的次数，选择正面频率最高的那个硬币，作为最佳策略。这也是大多人能想到的方法。

但是这个策略有几个明显问题：

10次尝试真的靠谱吗？最差的硬币也有可能在这10次内有高于最好硬币的正面次数。
假设你选到的这个硬币在投掷次数多了后发生了问题（比如掉屑），改变了其属性，导致其正面的概率大大降低，如果你还死守着它，那不是吃大亏了？（这是对变量的考虑）
就算你给一个硬币10次机会，如果硬币真的很多，比如K>100，给每个硬币10次机会是不是也太浪费了呢？等所有硬币都尝试过，再回来“赚钱”，花儿都谢了！

3. $\epsilon$-Greedy
有了前两个垫背，可以开始让Bandit登场了。ε-Greedy就是一种很机智的Bandit算法：它让每次机会以ε的概率去“探索”，1-ε的概率来“开发”。也即，如果一次机会落入ε中，则随机选择一个硬币来投掷，否则就选择先前探索到正面概率最大的硬币。这个策略有两个好处：

它能够应对变化，如果硬币“变质”了，它也能及时改变策略。
ε-Greedy机制让玩的过程更有趣，有时“探索”，有时“赚钱”。
在此基础上，又能引申出很多值得研究的问题，比如ε应该如何设定呢？它应不应该随着时间而变？因为随着探索次数的增多，好的选择自然浮现得比较明显了。ε大则使得模型有更大的灵活性（能更快的探索到未知，适应变化），ε小则会有更好的稳定性（有更多机会去“开发”）。

4. Thompson Sampling
每个臂是否产生收益的概率 p 的背后都对应一个 beta 分布。我们将 beta 分布的 α 参数看成是推荐后用户的点击次数， β 参数看成是推荐后用户未点击的次数。

来看下使用汤普森算法的流程：

1. 每个臂都维护一个 beta 分布的参数，获取每个臂对应的参数 α 和 β，然后使用 beta 分布生成随机数；
2. 选取生成随机数最大的那个臂作为本次结果；
3. 观察用户反馈，如果用户点击则将对应臂的 α 加 1，否则 β 加 1。
在实际的推荐系统中，需要为每个用户保存一套参数，假设有 m 个用户， n 个臂（选项，可以是物品，可以是策略）， 每个臂包含 α 和 β 两个参数，所以最后保存的参数的总个数是 2 m n。

可以直观的理解下为什么汤普森采样算法有效：

- 当尝试的次数较多时，即每个臂的 α + β 的值都很大，这时候每个臂对应的 beta 分布都会很窄，也就是说，生成的随机数都非常接近中心位置，每个臂的收益基本确定了。
- 当尝试的次数较少时，即每个臂的 α + β 的值都很小，这时候每个臂对应的 beta 分布都会很宽，生成的随机数有可能会比较大，增加被选中的机会。
- 当一个臂的 α + β 的值很大，并且 α/(α + β) 的值也很大，那么这个臂对应的 beta 分布会很窄，并且中心位置接近 1 ，那么这个臂每次选择时都很占优势。
```
import  numpy as np
import  pymc
# wins 和 trials 都是一个 N 维向量，N 是臂的个数
# wins 表示所有臂的 α 参数，loses 表示所有臂的 β 参数
choice = np.argmax(pymc.rbeta(1 + wins, 1 + loses, len(wins)))
wins[choice] += 1
loses[choice] += 1
```

5. UCB
UCB 算法全称是 Upper Confidence Bound，即置信区间上界。它是计算每个臂的平均收益与该收益的不确定性来作为最终得分。公式如下；
$$\overline{x_i(t)}+\sqrt(\frac{2*lnt}{T_{i,t}})$$
i 表示当前的臂，t 表示目前的尝试次数，Ti,t 表示臂 i 被选中的次数。公式加号左边表示臂 i 当前的平均收益，右边表示该收益的 Bonus ，本质上是均值的标准差，反应了候选臂效果的不确定性，就是置信区间的上边界。

使用 UCB 算法的流程如下：

- 对所有臂先尝试一次
- 按照公式计算每个臂的最终得分
- 选择得分最高的臂作为本次结果

直观理解下 UCB 算法为什么有效？

- 当一个臂的平均收益较大时，也就是公式左边较大，在每次选择时占有优势
- 当一个臂被选中的次数较少时，即 Tit 较小，那么它的 Bonus 较大，在每次选择时占有优势

所以 UCB 算法倾向选择被选中次数较少以及平均收益较大的臂。

UCB 算法需要对所有的臂进行一次尝试，当臂比较多时，可能会比较耗时，如果 UCB 算法的参数是确定的，那么输出结果就是确定的，也就是说它本质上仍然是一个“确定性”的算法，这会导致它的 explore 能力受限。

6. LinUCB（linear UCB）
- 《A Contextual-Bandit Approach to Personalized News Article Recommendation》
- 结合上下文信息


3. 参考资料
- http://www.naodongopen.com/903.html
- http://www.naodongopen.com/908.html
- http://blog.findshine.com/2015/01/03/bandit.html
- https://mp.weixin.qq.com/s?__biz=MzA4OTk5OTQzMg==&mid=2449231162&idx=1&sn=701246413bc0448b815803c0dc120f6d&scene=1&srcid=0623bwRRzpaEOOYGiYqYq3bK#rd
- https://cloud.tencent.com/developer/article/1042606