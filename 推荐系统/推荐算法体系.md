## 推荐算法体系

**入门圣经**
https://blog.csdn.net/dengxing1234/article/details/79756265

一. 算法架构
![算法架构](算法架构.png)

- 召回算法：包含了多个渠道的召回模型，如协同过滤，主题模型，内容召回和热点召回等，能够从数据库中选出多样性的偏好内容；
- 排序算法：对多个召回渠道的内容进行统一打分排序，选出最优的少量结果。

二. 技术细节
1. 召回算法（match）
- 基于内容
    - 关键字匹配
    - 文本分类
    - 文本主题
- 基于用户画像：用户标签匹配
- 协同过滤
    - user-based，item-based
    - 基于热门商品的降权
    - user-based优化
        - 将user-user-item 变为 user-expert-item
    - item-based优化
        - item聚类，形成topic
        - 将user-item-item 变为 user-topic-item
- lfm
    - 可解释性较差
    - biased-lfm有提升效果
- SVD，SVD++
- 受限玻尔兹曼机
- 关联规则：Apriori
- 其它
    1.1 基于用户行为的召回
        - 离线cf(ALS)
            利用基于itemBased的协同过滤生成离线的item2item相似度矩阵和用户的离线推荐结果，其中优化过程包括基于艾宾浩斯遗忘曲线按照时间进行降权、弱化热点影片的权重以及矩阵分解。
        - word2vec召回
            根据用户行为，将全部影片作为语料库，每个用户的观看历史按照时序排序视为文档，利用w2v算法原理计算所有item的词向量；
            根据词向量距离计算item2item的相似度矩阵
        - lda召回
            基于概率主题模型：文档-潜在主题-词的三级关系，映射到用户行为数据上，即用户-潜在兴趣-资源；
            通过用户历史行为记录，提取LDA的中间产物，即用户的潜在兴趣向量，以及资源的潜在主题分布向量
            基于item的主题向量，进行item2item的相似度计算
        - SimRank召回
            将user与deal的关系视作一个二部图，基于graph-based算法相似关系可以在图上传播的思想，使用simrank计算item相似队列
        - SVD，SVD++
        - Swing
        http://tech.meiyou.com/?p=471

    1.2 基于内容的召回
        - 基于title简介
            基于影片的文本简介部分使用doc2vector，计算出每个资源的表示向量
            基于资源的向量，进行item2item的相似度计算
        - 基于style(风格)
        - 基于tag(标签)

    1.3 内容+行为
        - Encoder-decoder
        
    1.4 向量召回
        - Session
        - Cross-domain
        - cold-start

2. 排序算法(rank)
2.1 baseline：lr
    - 优点：简单，解释性强，并行化程度高，实时预测性能可以得到保证
    - 缺点：需要大量的人工特征组合
2.2 FM
    - 优点：自动组合特征
    - 缺点：多级交叉复杂度高，难以建模深层语义
2.3 prerank：DSSM+规则
    rank：Wide&Deep
    postrank：DNN+规则  

2.4 GBDT+LR
    >GBDT是基于Boosting 思想的ensemble模型，由多颗决策树组成，具有以下优点：
    对输入特征的分布没有要求；
    根据熵增益自动进行特征转换、特征组合、特征选择和离散化，得到高维的组合特征，省去了人工转换的过程，并且支持了多个特征的Interaction；
    预测复杂度与特征个数无关。
    GBDT与LR的进行stacking可以一定程度防止GBDT过拟合。且升级为GBDT+LR可因为省去了对新特征进行人工转换的步骤，增加特征的迭代测试也相对容易。但是需要注意对于所有LR模型所有特征都进行离散化，出来的特征值全部非0即1。但是GBDT本来就是树模型，能很好的处理非线性特征，使用离散化后的特征效果可能不佳。而且对于这种只有0、1值的情况，GBDT可能出现了不收敛的现象。所以喂给GBDT的特征不建议进行没有必要的离散化。

2.5 GBDT+FM
    >GBDT+LR排序模型中输入特征维度为几百维，都是稠密的通用特征；
    这种特征的泛化能力良好，但是记忆能力比较差，所以需要增加高维的（百万维以上）内容特征来增强推荐的记忆能力，包括视频ID，标签，主题等特征。
    GBDT是不支持高维稀疏特征的，如果将高维特征加到LR中，一方面需要人工组合高维特征，另一方面模型维度和计算复杂度会是O（N^2）级别的增长。可采用Factorization Machines模型替换LR。FM能够自动对特征进行交叉组合、增加隐向量使得模型训练和预测的计算复杂度降为O(N)、支持稀疏特征这几个优点，FM使用GBDT（稠密特征）的叶子结点和稀疏特征（内容特征）作为输入。
    如果优化算法可以选择的话，建议FTRL，它较SGD有以下优势：带有L1正则使得学习的特征更加稀疏、使用累计的梯度加速收敛、根据特征在样本的出现频率确定该特征学习率，保证每个特征有充分的学习。FM模型中的特征出现的频次相差很大，FTRL能够保证每个特征都能得到充分的学习，更适合稀疏特征。

2.6 DNN+GBDT+FM
    > GBDT+FM模型，对embedding等具有结构信息的深度特征利用不充分，而深度学习（Deep Neural Network）能够对嵌入式（embedding）特征和普通稠密特征进行学习，抽取出深层信息，提高模型的准确性。
    DNN+GBDT+FM的ensemble模型FM层作为模型的最后一层，即融合层，其输入由三部分组成：DNN的最后一层隐藏层、GBDT的输出叶子节点、高维稀疏特征。
    DNN模型：（a）使用全连接网络，设置N个隐藏层。（b）预训练好的用户和视频的Embedding向量，包含基于用户行为以及基于语义内容等的多种Embedding，防止隐层不够，训练不足。（c）DNN能从具有良好数学分布的特征中抽取深层信息，比如embedding特征，归一化后统计特征等等。
    GBDT模型： （a）单独进行训练，输入包含归一化和未归一化的稠密特征（b）能处理未归一化的连续和离散特征。（c）能根据熵增益自动对输入特征进行离散和组合。
    FM融合层：（a）FM模型与DNN模型作为同一个网络同时训练。（b）将DNN特征，GBDT输出和稀疏特征进行融合并交叉
2.7 Wide & Deep
    Cheng, H. T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H., … & Anil, R. (2016, September). Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems (pp. 7-10). ACM.
    - DeepCross Network
    Wang, R., Fu, B., Fu, G., & Wang, M. (2017, August). Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 (p. 12). ACM. 
    - DeepFM
    Guo, H., Tang, R., Ye, Y., Li, Z., & He, X. (2017). Deepfm: a factorization-machine based neural network for ctr prediction. arXiv preprint arXiv:1703.04247.
    - xDeepFM
     Lian, J., Zhou, X., Zhang, F., Chen, Z., Xie, X., & Sun, G. (2018). xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. arXiv preprint arXiv:1803.05170.

2.8 融合
        - bagging：加权平均
        - 级联：模型的中间结果输出到另一个模型
        - Wide & Deep	
2.9 实时
        FTRL

2.10 组合不同的目标函数（多目标优化：mutil-task）
>考虑融合点击和播放时长不同性质的目标函数，进行模型训练：
（a）可独立成不同的模型进行训练。
（b）亦可不同的模型共享一定的深度隐层，最后在全连接层进行独立分割。最后在线上预测时对不同性质的模型，进行线性加权。


3. EE问题&冷启动问题
    - 冷启动用户召回
        计算资源得分(例如电影：根据imbd算法)，根据不同时间周期进行得分融合并线上进行ab对比，选取最优的时间周期组合
        按照imbd得分进行倒排，生成热点召回数据
    - 冷启动资源召回
        基于资源库，统计各个资源的点击和播放率，按一定比例召回点击/播放率低的item
    - EE问题
        method1：调整不同召回渠道的配比方式来保证一定的多样性
        method2：bandit算法
        method3：强化学习
    - 常用Bandit算法（累积遗憾->评估效果） （Multi-Armed Bandit）
    ![](Multi-Armed-Bandit.png)
    ![](Multi-Armed-Bandit2.png)

    >Thompson sampling算法
    UCB算法
    Epsilon-Greedy算法
    朴素Bandit算法
    LinUCB（UCB算法加入特征信息，可以实现个性化）
    COFIBA算法（Bandit和协同过滤结合）
    - Maximal Marginal Relevance
        - 强行加入一些非相关的推荐
        ![](Maximal_Marginal_Relevance.png)
    - Determinantal Point Process
        ![](Determinantal_Point_Process.png)
    - Social Curiosity Inspired Recommendation 


4. 特征工程
- 特征分类
    - 按基本来源分类
        - item特征：资源风格、地域、类型、标签、以及相应的统计特征等
        - user特征：性别、年龄、婚姻状况、收入预测等
        - context特征：网络状态、时间段、年龄等
        - 交叉特征：user/item/context交叉
    - 按更新频率分类
        - 离线特征：一段时间变化幅度小的特征，如user/item的基本特征和统计特征
        - 近在线特征：分钟级/小时级需要更新的特征，如item近4个小时的ctr等
        - 在线特征：需要在每次请求到达时，实时获取的特征，如网络状态、当前请求时间等

- 特征处理
    - 特征扩充
    >(1) 用户兴趣向量：lda的中间产物之一用户潜在兴趣向量，以及基于w2v的词向量和用户行为历史统计出的用户兴趣向量，可丰富用户维度上的兴趣特征。 
     (2) 资源embedding向量：基于用户行为数据进行的embedding的w2v/lda的词向量，以及基于资源本身title的doc2vector都可丰富最近模型使用的特征维度。且word2vecor和lda由于原理上的区别，可拓展出不同意义上的特征向量。 
     (3) 资源封面AutoEncode向量：基于资源封面图像，离线采用AutoEncode训练，提取隐层向量，作为资源本身特征。

    - 统计特征细化
    >(1) 特征工程时间窗口细化： 将资源的统计特征，按不同的时间窗口，如1,3,5,7,15天进行特征细化，可在丰富资源特征的同时，融入时间衰减的因素。 
    (2) 在线特征交叉：对于一些资源统计特征，在与用户特征以及上下文特征进行交叉之后效果会更加明显，根本原因在于增加了样本特征的区分度；具体交叉方法的落地，建议由服务端是执行，这里比如一个男性用户发来请求，服务端可实时获取用户的性别特征（男）以及召回队列中每个资源的在不同性别上统计特征（如历史CTR），基于特征编码表，新增加交叉特征(sex_cross_ctr)，并将每个资源在男性上的ctr上作为新增交叉特征的特征值，记录成log，后面回流给模型样本。其实所有的在线特征都和交叉特征一样，最后都由服务端的日志回流给模型训练过程。

    - 连续特征离散化 & 统一独热编码
    >(1) 当然，它归一化和这里离散化的方法的考虑点几乎一致。即由于不同维度特征的取值分布、相同维度下特征值的差异都很大。如很多特征的数据服从长尾分布。直接基于该特征进行常规的归一化方法（例如 min-max， z-score）都只是对数据的分布进行平移和拉伸，最后特征的分布仍然是长尾分布，这就导致大部分样本的特征值都集中在非常小的取值范围内，使得样本特征的区分度减小；与此同时，少量的大值特征可能造成训练时的波动，减缓收敛速度。此外也可以对特征值做对数转化，但由于不同维度间特征的分布不同，这种特征值处理的方式并不一定适用于其他维度的特征。 
    https://tech.meituan.com/recommend_dnn.html
     (2) **考虑到LR模型的特性，这里的方法是先对连续特征先进行等频离散化，即基于特征值的频率等频分成N个桶，并做0/1标识，最后和离散型特征，统一进行One-hot编码，喂给LR模型**。
     **在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型**，这样做的优势有以下几点：
     0. 离散特征的增加和减少都很容易，易于模型的快速迭代；
     1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
     2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
     3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
     4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
     5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
     6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

     李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

    https://www.zhihu.com/question/31989952/answer/54184582

5. 工程架构
- offline
    - 运算时间小时计
    - 推荐数目：千
    - 大数据集群
- nearline
    - 运算时间秒计
    - 推荐数目：百
    - 内存+redis
- online
    - 运算时间毫秒计
    - 推荐数目：十
    - 基于内存运算

6. 提升
- 建立评估体系
    - 线下:MAP
    https://blog.csdn.net/simple_the_best/article/details/52296608
    - 线上
- 长短期兴趣画像
    - 过滤：历史推荐过的，质量不高，不满足用户需求的过滤掉
    - 排序：引入时间因子，捕捉用户短期兴趣偏好，采用机器学习方法灵活调整推荐顺序
- Embedding处理
    - 词Embedding处理：内容多样化
    - item embedding：结果召回
    - user和item的向量embedding，用作特征
- 基于知识图谱的推荐
    - 解决用户行为数据稀疏性的问题，把数据变稠密，同时推荐结果增加多样性
- 采样策略
    - 负样本采样策略调整
        基于曝光时间showtime和曝光顺序order，过滤负样本。
    - 不平衡样板策略调整
        离线进行A/B测试不同正负样本比例，进行择优调整（如1:5）
- 召回算法探索
    - 可以利用RNN来“捕捉”用户在点击序列中的模式，即利用用户点击行为发生先后顺序进行推荐的展示排序。
    - Graph embedding算法
    https://blog.csdn.net/b0q8cpra539hafs7/article/details/78869969
    - 强化学习
    https://m.aliyun.com/yunqi/articles/66158


三.总结

- 按照目前主流的算法架构 match + rank，不难发现match召回算法最终决定了推荐效果的上界，而rank层的排序是更加保证了推荐结果的精准。所以从模型优化的角度来讲，只有保证match 和 rank 双管齐下，才能发挥推荐系统的终极效果。match趋近个性化、召回度和新颖度的完美结合，rank层趋近排序结果的精准化，无疑对于算法工程师来说，都是极大的挑战。
- 基于bad case去提升算法，没有绝对好的算法，只有适用的算法
