# 交叉熵 (cross-entropy)

1. 信息量
- -log(p)
- 概率越小的事情发生了，信息量越大

2. 熵:信息量的期望
$$H(x) = -\sum\limits_{i = 1}^{n}p(xi)log(p(xi))$$

3. 相对熵(KL散度)
- 如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异
- In the context of machine learning, DKL(P‖Q) is often called the information gain achieved if P is used instead of Q.
- 在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1] 
$$D_{KL}(p||q) = \sum\limits_{i=1}^{n}p(xi)log(\frac{p(xi)}{q(xi)})$$
- $D_{KL}$的值越小，表示q分布和p分布越接近

4. 交叉熵
$$H(p,q) = \sum\limits_{i=1}^{n}p(xi)log(q(xi))$$
$$D_{KL}(p||q) =  -H(p(x))+H(p,q)$$

**在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即DKL(y||y^)DKL(y||y^)D_{KL}(y||\hat{y})，由于KL散度中的前一部分−H(y)−H(y)-H(y)不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。**



