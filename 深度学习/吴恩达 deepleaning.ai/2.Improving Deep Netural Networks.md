## week1 深度学习的实用层面
1.1. train/dev/test
- 数据量如果很大，可以降低dev/test的数据比例
- 验证集和测试集should come from same distribution
- 也可以不要测试集
1.2 Bias and Variance
- 如何判断bias、variance
  假设最优误差是0%
  训练集误差1%，验证集误差15% -- high variance
  训练集误差15%，验证集误差16% -- high bias
  训练集误差15%，验证集误差30% -- high bias,high variance  （欠拟合部分数据，过度拟合部分数据）
  训练集误差0.5%，验证集误差1% -- low bias,low variance
  
- High Bias:under fitting -- bigger network，more iterations
  High Variance:over fitting -- more data,regularization
- 深度学习时代，bias-variance trade-off 经常不再是一个问题，可以在降低bias的同时不增大variance，反之亦然

1.3 正则化
- 降低variance
- L1正则可以起到特征选择的作用 （参数W will be sparse）
- L2正则在深度学习中使用更多
- L2正则也叫“权重衰减”(weight-decay):梯度下降过程中参数w会越来越小
- dropout： 对于每一个样本，都以一定概率消除网络节点，以一个精简的网络结构来训练模型
  - 实现：Invertedd dropout（反向随机失活）
  - 预测阶段：no drop out
  - 因为输入随时可能被消灭，因此drop out起到了降低权重的作用
  - 不同层的保留概率(keep-prop)可以不同
  - 主要用于计算机视觉领域：数据不足，容易过拟合
  - 缺点：cost function 缺乏明确的定义
- 其它降低过拟合的方法
  - 增加数据:图片翻转、旋转、剪裁
  - early stopping ： 当验证集误差不再减小，提前终止训练

1.4 归一化输入(Normalizing)
- 加速训练
- step1：减去平均值
- step2：除以方差和
- 训练集和测试集要一起归一化
- 优点：特征值范围类似，代价函数会更圆一些，学习速度会很快

1.5 梯度消失、梯度爆炸
- 导数指数式增大、减小
- 层数越多，问题越严重
- 解决办法：初始化权重使得权重不是太大，也不是太小
  - 如对于ReLU：np.random.randn(shape)*np.sqrt(2/n)
  - 如对于tanh：Xavier initialization

1.6 梯度的数值逼近
- 双边误差比单边误差更精确

## week2 优化算法
2.1 mini-batch
- 随着迭代的进行，cost function会有波动，不过趋势是减小的 
- choose mini-batch size：64~512
  
2.2 指数加权平均
$$v_t=\beta*v_{t-1}+(1-\beta)\theta_t$$
- $$\beta=0.1$$，近似等于计算最近10天的平均值，优点是占内存很小
- bias correction(偏差修正)：初期估算不准
  $$v_t -> \frac{v_t}{(1-\beta^t)}$$

2.3 Momentum

2.4 









## week3 超参数调试、Batch正则化和程序框架