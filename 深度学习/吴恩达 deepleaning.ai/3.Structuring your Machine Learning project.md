# 3. Structuring your Machine Learning project

## week1.机器学习策略1
1.1 正交化 
按顺序调节以下目标，一个一个来
- 训练集上表现良好
- 验证集上表现良好
- 测试集上表现良好
- 真实场景中表现良好

“early stopping”不是很正交，会同时影响训练集和验证集的表现

1.2 单实数评价指标

1.3 change metrics and dev/test set
- 如果在dev/test 上表现良好，实际应用表现不佳，可以考虑change metrics and dev/test set

## week2.机器学习策略2
2.1 误差分析
- 统计不同类型错误标记的例子的分布，可以提高优化的效率

2.2 训练集有错误标注怎么办
- DL algorithms are quite robust to **random errors** in the training set
    - 错误标记不要太多就行
- not robust to **systemetic errors** 
    - 比如一直把白色的狗标记成猫

2.3 快速搭建系统，快速迭代
- set up train/dev/test set and metric
- build initial system quickly
- use bias/variance analysis and error analysis to prioritize next steps

2.4 训练集和测试集来自不同的分布
- 深度学习需要大量数据，因此训练集数据越多越好，从各种渠道采集
- 这种情况下，不要将训练集和验证集混合，因为这样会降低验证集中真实样本的比例
- bias & variance：train/train-dev/dev/test set
    - train-dev来自训练集，可以验证是否存在variance过大的问题
    - human level/training set error/training-dev set error/dev error/test error 
        --- avoidable bias/variance/data mismatch/degree of overfiting to dev set
- data mismatch problem ： 了解训练集和测试集的差异，使训练集尽量接近测试集（比如人工加噪声）

2.5 迁移学习
- step1：在大数据集上训练成熟网络
- step2：去除最后一层
- step3：在新数据集上重新训练部分层
- when transfer learning makes sense
    - Task A and B have the same input x
    - You have a lot more data for Task A than Task B
    - Low level features from A could be helpful for learning B

2.6 多任务学习
- 目标变量y是多维的，最后一层也是多维的
- when multi-task learning makes sense
    - Training on a set of tasks that could benefit from having shared lower-level features
    - Usually:Amount of data you have for each task is quite similar
    - Can train a big enough netural network to do well on all the tasks

2.7 end-to-end deep learning
- 不需要数据处理/特征提取等中间流水线步骤
- 但是需要更多的数据
- 如果数据没那么多，可以把问题拆成多步，这样每一步都有足够的数据
- Pros:
    - Let the data speak:让数据表现出自己的信息，而不是引入人类的成见
    - Less hand-designing of components needed
- Cons:
    - need large amount of data
    - Exclude potentially useful hand-designed components

