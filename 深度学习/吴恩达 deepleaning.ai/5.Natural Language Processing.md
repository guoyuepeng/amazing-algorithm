## week1.循环序列模型
1.1 词表示
- 词袋
- 

1.2 RNN
- DNN为什么不能解决？
    - inputs，outputs can be different lengths in different examples
    - doesn't share feture learned from different positions of text
- RNN
    - use feature from different positions of text
    - tanh : common choice
- different types of RNN
    - many to many ：翻译
    - many to one : 情感分类
    - one to many : 音乐生成

1.3 language modeling
- 计算给定语句的概率
- build model
    - word-level,character-level
    - 构建语料库
    - 分词，并将词对应到字典(Tokenize)
    - RNN model : y -- 这个位置是这个词的概率

1.4 **sampling novel sequences**
- sampling a sequence from a trained RNN
- 机器写文章！

1.5 Vanishing gradients in RNN 
- 很难有长期记忆
- 梯度爆炸的问题容易被发现，可以用梯度修建解决：给梯度设置一个阈值

1.6 GRU：Gated Recurrent Unit
- C = memory cell
- 视觉门，相关门

1.7 LSTM
- more powerful

## week2.自然语言处理与词嵌入



## week3.序列模型和注意力机制